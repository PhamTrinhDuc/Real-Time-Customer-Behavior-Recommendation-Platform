Tuyá»‡t vá»i! DÆ°á»›i Ä‘Ã¢y lÃ  **outline Ä‘áº§y Ä‘á»§, chi tiáº¿t tá»«ng bÆ°á»›c tá»« khÃ¢u dá»¯ liá»‡u â†’ xá»­ lÃ½ â†’ ML â†’ triá»ƒn khai**, giÃºp báº¡n xÃ¢y dá»±ng má»™t **dá»± Ã¡n end-to-end hoÃ n chá»‰nh**, **thá»±c táº¿**, **khÃ´ng cáº§n Ä‘iá»u kiá»‡n tiÃªn quyáº¿t**, vÃ  **cá»±c ká»³ áº¥n tÆ°á»£ng khi Ä‘i xin viá»‡c**.

---

# ğŸ§­ **Dá»° ÃN: REAL-TIME CUSTOMER BEHAVIOR & RECOMMENDATION PLATFORM**

> **Má»¥c tiÃªu**: XÃ¢y dá»±ng há»‡ thá»‘ng data pipeline tá»« OLTP â†’ Data Lakehouse â†’ Feature Store â†’ Recommendation Model â†’ Serving â†’ Dashboard â€” táº¥t cáº£ cháº¡y local vá»›i Docker.

---

## ğŸ¯ Má»¤C TIÃŠU CHIáº¾N LÆ¯á»¢C

| Má»¥c tiÃªu | Chi tiáº¿t |
|---------|----------|
| âœ… **Thá»±c táº¿** | Giáº£i quyáº¿t bÃ i toÃ¡n: â€œGá»£i Ã½ sáº£n pháº©m cho khÃ¡ch hÃ ng dá»±a trÃªn hÃ nh vi mua hÃ ngâ€ |
| âœ… **ToÃ n diá»‡n** | Cover: CDC, Streaming, Batch, Orchestration, Feature Store, ML, Serving, Dashboard |
| âœ… **KhÃ´ng cáº§n cloud** | Cháº¡y hoÃ n toÃ n local vá»›i Docker |
| âœ… **Showcase ká»¹ nÄƒng** | End-to-end pipeline â€” cá»±c ká»³ quÃ½ giÃ¡ vá»›i nhÃ  tuyá»ƒn dá»¥ng Data Engineer / MLOps |

---

# ğŸ—ºï¸ OUTLINE CHI TIáº¾T Tá»ªNG GIAI ÄOáº N

---

## ğŸ§± PHASE 0: THIáº¾T Láº¬P MÃ”I TRÆ¯á»œNG (Tuáº§n 0)

### âœ… CÃ´ng cá»¥ cáº§n cÃ i:
- Docker + Docker Compose
- Python 3.9+
- Spark (local mode)
- Flink (local mode)
- Git + GitHub

### âœ… `docker-compose.yml` (gá»“m cÃ¡c service):

```yaml
version: '3.8'
services:
  postgres:
    image: debezium/postgres:15
    ports: ["5432:5432"]
    environment:
      POSTGRES_DB: ecommerce
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres

  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment: ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on: [zookeeper]
    ports: ["9092:9092"]
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  debezium:
    image: debezium/connect:2.4
    depends_on: [kafka, postgres]
    ports: ["8083:8083"]
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: connect_configs
      OFFSET_STORAGE_TOPIC: connect_offsets

  minio:
    image: minio/minio
    ports: ["9000:9000", "9001:9001"]
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123

  trino:
    image: trinodb/trino
    ports: ["8080:8080"]
    volumes: ["./trino/catalog:/usr/lib/trino/etc/catalog"]

  redis:
    image: redis:7
    ports: ["6379:6379"]

  airflow:
    image: apache/airflow:2.8.0
    ports: ["8081:8080"]
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://postgres:postgres@postgres/airflow
    depends_on: [postgres]
    volumes: ["./dags:/opt/airflow/dags"]
```

> ğŸ’¡ MÃ¬nh sáº½ gá»­i báº¡n file `docker-compose.yml` hoÃ n chá»‰nh + hÆ°á»›ng dáº«n khá»Ÿi cháº¡y náº¿u cáº§n.

---

## ğŸ“Š PHASE 1: Táº O Dá»® LIá»†U GIáº¢ Láº¬P (Tuáº§n 1)

### âœ… Schema PostgreSQL (4 báº£ng chÃ­nh)

```sql
-- users
CREATE TABLE users (user_id SERIAL PRIMARY KEY, name TEXT, email TEXT, created_at TIMESTAMP, last_login TIMESTAMP);

-- products
CREATE TABLE products (product_id SERIAL PRIMARY KEY, name TEXT, price DECIMAL(10,2), category TEXT);

-- orders
CREATE TABLE orders (order_id SERIAL PRIMARY KEY, user_id INT, total_amount DECIMAL(10,2), status TEXT, created_at TIMESTAMP);

-- order_items
CREATE TABLE order_items (order_item_id SERIAL PRIMARY KEY, order_id INT, product_id INT, quantity INT, price_per_unit DECIMAL(10,2));
```

### âœ… Script Python táº¡o dá»¯ liá»‡u (generate_data.py)

```python
# generate_data.py
from faker import Faker
import psycopg2
import random

fake = Faker()

conn = psycopg2.connect(dbname="ecommerce", user="postgres", password="postgres", host="localhost")
cur = conn.cursor()

# Insert 100 users
for _ in range(100):
    cur.execute("""
        INSERT INTO users (name, email, created_at, last_login)
        VALUES (%s, %s, NOW() - INTERVAL '%s days', NOW())
    """, (fake.name(), fake.email(), random.randint(1, 365)))

# Insert 50 products
categories = ['Electronics', 'Books', 'Clothing', 'Home']
for _ in range(50):
    cur.execute("""
        INSERT INTO products (name, price, category)
        VALUES (%s, %s, %s)
    """, (fake.word(), round(random.uniform(10, 500), 2), random.choice(categories)))

conn.commit()
print("âœ… ÄÃ£ táº¡o dá»¯ liá»‡u máº«u!")
```

### âœ… Script mÃ´ phá»ng mua hÃ ng liÃªn tá»¥c (simulate_orders.py)

```python
# simulate_orders.py
import time
import random
import psycopg2

conn = psycopg2.connect(dbname="ecommerce", user="postgres", password="postgres", host="localhost")
cur = conn.cursor()

while True:
    user_id = random.randint(1, 100)
    product_id = random.randint(1, 50)
    quantity = random.randint(1, 5)
    price = round(random.uniform(10, 500), 2)

    # Insert order
    cur.execute("""
        INSERT INTO orders (user_id, total_amount, status, created_at)
        VALUES (%s, %s, 'completed', NOW())
        RETURNING order_id
    """, (user_id, quantity * price))
    order_id = cur.fetchone()[0]

    # Insert order item
    cur.execute("""
        INSERT INTO order_items (order_id, product_id, quantity, price_per_unit)
        VALUES (%s, %s, %s, %s)
    """, (order_id, product_id, quantity, price))

    conn.commit()
    print(f"ğŸ›’ ÄÃ£ táº¡o Ä‘Æ¡n hÃ ng #{order_id} cho user {user_id}")
    time.sleep(3)  # 1 Ä‘Æ¡n má»—i 3 giÃ¢y
```

> ğŸ’¡ Cháº¡y script nÃ y Ä‘á»ƒ dá»¯ liá»‡u â€œcháº£yâ€ liÃªn tá»¥c â†’ Debezium báº¯t event â†’ Kafka â†’ Flink xá»­ lÃ½ real-time.

---

## ğŸ”„ PHASE 2: CDC + STREAMING PIPELINE (Tuáº§n 2)

### âœ… Cáº¥u hÃ¬nh Debezium (qua REST API)

```bash
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d '{
    "name": "ecommerce-connector",
    "config": {
      "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
      "database.hostname": "postgres",
      "database.port": "5432",
      "database.user": "postgres",
      "database.password": "postgres",
      "database.dbname": "ecommerce",
      "database.server.name": "ecommerce",
      "table.include.list": "public.users,public.products,public.orders,public.order_items",
      "plugin.name": "pgoutput"
    }
  }'
```

â†’ Kiá»ƒm tra Kafka topics: `ecommerce.public.orders`, `ecommerce.public.order_items`

### âœ… Flink Streaming Job (realtime_user_behavior.py)

```python
# realtime_user_behavior.py
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment

env = StreamExecutionEnvironment.get_execution_environment()
t_env = StreamTableEnvironment.create(env)

t_env.execute_sql("""
    CREATE TABLE order_events (
        order_id BIGINT,
        user_id BIGINT,
        total_amount DECIMAL(10,2),
        created_at TIMESTAMP(3),
        proc_time AS PROCTIME()
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'ecommerce.public.orders',
        'properties.bootstrap.servers' = 'localhost:9092',
        'format' = 'json',
        'scan.startup.mode' = 'earliest-offset'
    )
""")

# TÃ­nh tá»•ng doanh thu theo phÃºt
t_env.execute_sql("""
    CREATE TABLE user_behavior_sink (
        window_start TIMESTAMP(3),
        total_revenue DECIMAL(10,2)
    ) WITH (
        'connector' = 'print'
    )
""")

t_env.execute_sql("""
    INSERT INTO user_behavior_sink
    SELECT
        TUMBLE_START(proc_time, INTERVAL '1' MINUTE) as window_start,
        SUM(total_amount) as total_revenue
    FROM order_events
    GROUP BY TUMBLE(proc_time, INTERVAL '1' MINUTE)
""")
```

â†’ Output ra console hoáº·c ghi vÃ o Kafka topic má»›i: `realtime_revenue`

---

## ğŸï¸ PHASE 3: DATA LAKEHOUSE + BATCH ANALYTICS (Tuáº§n 3)

### âœ… Spark Job: Ghi dá»¯ liá»‡u tá»« Kafka â†’ Delta Lake (MinIO)

```python
# spark_to_deltalake.py
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("KafkaToDeltaLake") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minio") \
    .config("spark.hadoop.fs.s3a.secret.key", "minio123") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .getOrCreate()

df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "ecommerce.public.orders") \
    .load()

df_parsed = df.selectExpr("CAST(value AS STRING) as json") \
    .selectExpr("from_json(json, 'order_id LONG, user_id LONG, total_amount DOUBLE, created_at TIMESTAMP') as data") \
    .select("data.*")

df_parsed.writeStream \
    .format("delta") \
    .option("checkpointLocation", "s3a://lakehouse/checkpoints/orders") \
    .start("s3a://lakehouse/raw/orders")
```

### âœ… Trino: Cáº¥u hÃ¬nh catalog Ä‘á»ƒ query Delta Lake

File: `trino/catalog/delta.properties`

```properties
connector.name=delta-lake
hive.metastore.uri=thrift://localhost:9083
delta.enable-non-concurrent-writes=true
```

â†’ Query: `SELECT * FROM delta."s3a://lakehouse/raw/orders" LIMIT 10;`

---

## ğŸ§® PHASE 4: FEATURE ENGINEERING + FEATURE STORE (Tuáº§n 4)

### âœ… Spark Feature Jobs (feature_engineering.py)

```python
# feature_engineering.py
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("FeatureEngineering").getOrCreate()

# 1. User-Item Interaction
df_interactions = spark.sql("""
    SELECT user_id, product_id, SUM(quantity) as interaction_score
    FROM delta.`s3a://lakehouse/raw/order_items` oi
    JOIN delta.`s3a://lakehouse/raw/orders` o ON oi.order_id = o.order_id
    GROUP BY user_id, product_id
""")

# 2. Product Popularity
df_popularity = spark.sql("""
    SELECT product_id, COUNT(*) as purchase_count, AVG(price_per_unit) as avg_price
    FROM delta.`s3a://lakehouse/raw/order_items`
    GROUP BY product_id
""")

# 3. User RFM
df_rfm = spark.sql("""
    SELECT user_id,
           DATEDIFF(CURRENT_DATE, MAX(created_at)) as recency,
           COUNT(*) as frequency,
           SUM(total_amount) as monetary
    FROM delta.`s3a://lakehouse/raw/orders`
    GROUP BY user_id
""")

# LÆ°u vÃ o Delta Lake
df_interactions.write.format("delta").mode("overwrite").save("s3a://lakehouse/features/user_item_interactions")
df_popularity.write.format("delta").mode("overwrite").save("s3a://lakehouse/features/product_popularity")
df_rfm.write.format("delta").mode("overwrite").save("s3a://lakehouse/features/user_rfm")
```

### âœ… Airflow DAG: Tá»± Ä‘á»™ng cáº­p nháº­t Feature Store

```python
# dags/update_features.py
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

with DAG('update_feature_store', start_date=datetime(2025, 1, 1), schedule_interval='@daily') as dag:

    run_spark_features = BashOperator(
        task_id='run_feature_engineering',
        bash_command='spark-submit /opt/airflow/dags/feature_engineering.py'
    )

    load_to_redis = BashOperator(
        task_id='load_features_to_redis',
        bash_command='python /opt/airflow/dags/load_to_redis.py'
    )

    run_spark_features >> load_to_redis
```

### âœ… Script Ä‘áº©y feature vÃ o Redis (load_to_redis.py)

```python
# load_to_redis.py
import redis
import json
from pyspark.sql import SparkSession

r = redis.Redis(host='redis', port=6379, db=0)

spark = SparkSession.builder.appName("LoadToRedis").getOrCreate()

df = spark.read.format("delta").load("s3a://lakehouse/features/user_item_interactions")

for row in df.collect():
    key = f"user:{row['user_id']}:interactions"
    value = json.dumps({"product_id": row['product_id'], "score": row['interaction_score']})
    r.hset(key, row['product_id'], value)

print("âœ… ÄÃ£ cáº­p nháº­t feature store vÃ o Redis!")
```

---

## ğŸ¤– PHASE 5: ML RECOMMENDATION MODEL (Tuáº§n 5)

### âœ… Train ALS Model (train_als.py)

```python
# train_als.py
from pyspark.ml.recommendation import ALS
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("TrainALS").getOrCreate()

df = spark.read.format("delta").load("s3a://lakehouse/features/user_item_interactions")

als = ALS(
    maxIter=10,
    regParam=0.01,
    userCol="user_id",
    itemCol="product_id",
    ratingCol="interaction_score",
    coldStartStrategy="drop"
)

model = als.fit(df)
model.write().overwrite().save("s3a://lakehouse/models/als_recommender")

print("âœ… ÄÃ£ lÆ°u mÃ´ hÃ¬nh ALS vÃ o Delta Lake!")
```

### âœ… Serve Recommendation (serve_recommendation.py)

```python
# serve_recommendation.py
import redis
from pyspark.ml.recommendation import ALSModel
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("ServeRecommendation").getOrCreate()
model = ALSModel.load("s3a://lakehouse/models/als_recommender")

r = redis.Redis(host='redis', port=6379, db=0)

def recommend_for_user(user_id, top_k=5):
    user_df = spark.createDataFrame([(user_id,)], ["user_id"])
    recommendations = model.recommendForUserSubset(user_df, top_k)
    rec_list = recommendations.collect()[0]["recommendations"]
    return [r["product_id"] for r in rec_list]

# VÃ­ dá»¥: Gá»£i Ã½ cho user 123
print(recommend_for_user(123))
```

---

## ğŸ“ˆ PHASE 6: DASHBOARD & MONITORING (Tuáº§n 6)

### âœ… DÃ¹ng Trino + Metabase (hoáº·c Superset)

- Trino query dá»¯ liá»‡u tá»« Delta Lake â†’ Metabase visualize
- Dashboard gá»“m:
  - Doanh thu theo thá»i gian
  - Top sáº£n pháº©m bÃ¡n cháº¡y
  - PhÃ¢n bá»‘ khÃ¡ch hÃ ng theo RFM
  - Sá»‘ lÆ°á»£ng Ä‘á» xuáº¥t Ä‘Æ°á»£c sinh ra

### âœ… Log & Alert (Airflow)

- Gá»­i Slack/Email náº¿u job fail
- Ghi log vÃ o file hoáº·c Elasticsearch (optional)

---

## ğŸ“ PHASE 7: DOCUMENT & PRESENT (Tuáº§n 7)

### âœ… GitHub Repo Structure

```
/customer-insight-platform/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ generate_data.py
â”‚   â””â”€â”€ simulate_orders.py
â”œâ”€â”€ streaming/
â”‚   â””â”€â”€ realtime_user_behavior.py
â”œâ”€â”€ batch/
â”‚   â””â”€â”€ feature_engineering.py
â”œâ”€â”€ ml/
â”‚   â”œâ”€â”€ train_als.py
â”‚   â””â”€â”€ serve_recommendation.py
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ update_features.py
â”‚   â””â”€â”€ load_to_redis.py
â””â”€â”€ docs/
    â””â”€â”€ architecture.png
```

### âœ… README.md pháº£i cÃ³:

- MÃ´ táº£ dá»± Ã¡n
- CÃ¡ch cháº¡y (docker-compose up, python scripts, airflow)
- Screenshot dashboard
- Kiáº¿n trÃºc há»‡ thá»‘ng (váº½ báº±ng draw.io hoáº·c Excalidraw)

---

## ğŸ“ KHI PHá»NG Váº¤N â€” Báº N NÃ“I GÃŒ?

> â€œTÃ´i Ä‘Ã£ tá»± xÃ¢y dá»±ng má»™t há»‡ thá»‘ng phÃ¢n tÃ­ch hÃ nh vi khÃ¡ch hÃ ng vÃ  gá»£i Ã½ sáº£n pháº©m end-to-end, bao gá»“m:  
> - Thu tháº­p dá»¯ liá»‡u real-time tá»« PostgreSQL qua Debezium + Kafka  
> - Xá»­ lÃ½ streaming báº±ng Flink  
> - LÆ°u trá»¯ vÃ o Delta Lake trÃªn MinIO  
> - Tá»± Ä‘á»™ng hÃ³a báº±ng Airflow  
> - XÃ¢y feature store vá»›i Redis + Delta Lake  
> - Train mÃ´ hÃ¬nh ALS recommendation báº±ng Spark MLlib  
> - Serve Ä‘á» xuáº¥t real-time  
> Táº¥t cáº£ cháº¡y local vá»›i Docker â€” khÃ´ng cáº§n cloud.â€

---

## âœ… BONUS: NÃ‚NG Cáº¤P SAU NÃ€Y (Náº¿u cÃ³ thá»i gian)

- ThÃªm Kafka Streams thay Flink
- DÃ¹ng Feast lÃ m Feature Store chuyÃªn nghiá»‡p
- Deploy model vá»›i FastAPI + Docker
- ThÃªm Grafana Ä‘á»ƒ monitor pipeline
- DÃ¹ng dbt Ä‘á»ƒ transform data

---

## ğŸš€ Báº N MUá»N MÃŒNH Gá»¬I GÃŒ TIáº¾P THEO?

ğŸ‘‰ MÃ¬nh cÃ³ thá»ƒ gá»­i báº¡n ngay:

- File `docker-compose.yml` hoÃ n chá»‰nh
- Script `generate_data.py`, `simulate_orders.py`
- Code Flink + Spark + Airflow máº«u
- File cáº¥u hÃ¬nh Trino + MinIO
- README.md máº«u chuyÃªn nghiá»‡p

Chá»‰ cáº§n nÃ³i: **â€œGá»­i code máº«uâ€** â€” mÃ¬nh sáº½ zip vÃ  gá»­i tá»«ng pháº§n theo thá»© tá»± báº¡n cáº§n!

---

Báº¡n Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ báº¯t Ä‘áº§u chÆ°a? MÃ¬nh sáº½ Ä‘á»“ng hÃ nh cÃ¹ng báº¡n tá»«ng bÆ°á»›c ğŸ˜Š