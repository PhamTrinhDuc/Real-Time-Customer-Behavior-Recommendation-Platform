Tuy·ªát v·ªùi! D∆∞·ªõi ƒë√¢y l√† **outline ƒë·∫ßy ƒë·ªß, chi ti·∫øt t·ª´ng b∆∞·ªõc t·ª´ kh√¢u d·ªØ li·ªáu ‚Üí x·ª≠ l√Ω ‚Üí ML ‚Üí tri·ªÉn khai**, gi√∫p b·∫°n x√¢y d·ª±ng m·ªôt **d·ª± √°n end-to-end ho√†n ch·ªânh**, **th·ª±c t·∫ø**, **kh√¥ng c·∫ßn ƒëi·ªÅu ki·ªán ti√™n quy·∫øt**, v√† **c·ª±c k·ª≥ ·∫•n t∆∞·ª£ng khi ƒëi xin vi·ªác**.

---

# üß≠ **D·ª∞ √ÅN: REAL-TIME CUSTOMER BEHAVIOR & RECOMMENDATION PLATFORM**

> **M·ª•c ti√™u**: X√¢y d·ª±ng h·ªá th·ªëng data pipeline t·ª´ OLTP ‚Üí Data Lakehouse ‚Üí Feature Store ‚Üí Recommendation Model ‚Üí Serving ‚Üí Dashboard ‚Äî t·∫•t c·∫£ ch·∫°y local v·ªõi Docker.

---

## üéØ M·ª§C TI√äU CHI·∫æN L∆Ø·ª¢C

| M·ª•c ti√™u | Chi ti·∫øt |
|---------|----------|
| ‚úÖ **Th·ª±c t·∫ø** | Gi·∫£i quy·∫øt b√†i to√°n: ‚ÄúG·ª£i √Ω s·∫£n ph·∫©m cho kh√°ch h√†ng d·ª±a tr√™n h√†nh vi mua h√†ng‚Äù |
| ‚úÖ **To√†n di·ªán** | Cover: CDC, Streaming, Batch, Orchestration, Feature Store, ML, Serving, Dashboard |
| ‚úÖ **Kh√¥ng c·∫ßn cloud** | Ch·∫°y ho√†n to√†n local v·ªõi Docker |
| ‚úÖ **Showcase k·ªπ nƒÉng** | End-to-end pipeline ‚Äî c·ª±c k·ª≥ qu√Ω gi√° v·ªõi nh√† tuy·ªÉn d·ª•ng Data Engineer / MLOps |

---

# üó∫Ô∏è OUTLINE CHI TI·∫æT T·ª™NG GIAI ƒêO·∫†N

---

## üß± PHASE 0: THI·∫æT L·∫¨P M√îI TR∆Ø·ªúNG (Tu·∫ßn 0)

### ‚úÖ C√¥ng c·ª• c·∫ßn c√†i:
- Docker + Docker Compose
- Python 3.9+
- Spark (local mode)
- Flink (local mode)
- Git + GitHub

### ‚úÖ `docker-compose.yml` (g·ªìm c√°c service):

```yaml
version: '3.8'
services:
  postgres:
    image: debezium/postgres:15
    ports: ["5432:5432"]
    environment:
      POSTGRES_DB: ecommerce
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres

  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment: ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on: [zookeeper]
    ports: ["9092:9092"]
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  debezium:
    image: debezium/connect:2.4
    depends_on: [kafka, postgres]
    ports: ["8083:8083"]
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: connect_configs
      OFFSET_STORAGE_TOPIC: connect_offsets

  minio:
    image: minio/minio
    ports: ["9000:9000", "9001:9001"]
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123

  trino:
    image: trinodb/trino
    ports: ["8080:8080"]
    volumes: ["./trino/catalog:/usr/lib/trino/etc/catalog"]

  redis:
    image: redis:7
    ports: ["6379:6379"]

  airflow:
    image: apache/airflow:2.8.0
    ports: ["8081:8080"]
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://postgres:postgres@postgres/airflow
    depends_on: [postgres]
    volumes: ["./dags:/opt/airflow/dags"]
```

> üí° M√¨nh s·∫Ω g·ª≠i b·∫°n file `docker-compose.yml` ho√†n ch·ªânh + h∆∞·ªõng d·∫´n kh·ªüi ch·∫°y n·∫øu c·∫ßn.

---

## üìä PHASE 1: T·∫†O D·ªÆ LI·ªÜU GI·∫¢ L·∫¨P (Tu·∫ßn 1)

### ‚úÖ Schema PostgreSQL (4 b·∫£ng ch√≠nh)

```sql
-- users
CREATE TABLE users (user_id SERIAL PRIMARY KEY, name TEXT, email TEXT, created_at TIMESTAMP, last_login TIMESTAMP);

-- products
CREATE TABLE products (product_id SERIAL PRIMARY KEY, name TEXT, price DECIMAL(10,2), category TEXT);

-- orders
CREATE TABLE orders (order_id SERIAL PRIMARY KEY, user_id INT, total_amount DECIMAL(10,2), status TEXT, created_at TIMESTAMP);

-- order_items
CREATE TABLE order_items (order_item_id SERIAL PRIMARY KEY, order_id INT, product_id INT, quantity INT, price_per_unit DECIMAL(10,2));
```

### ‚úÖ Script Python t·∫°o d·ªØ li·ªáu (generate_data.py)

```python
# generate_data.py
from faker import Faker
import psycopg2
import random

fake = Faker()

conn = psycopg2.connect(dbname="ecommerce", user="postgres", password="postgres", host="localhost")
cur = conn.cursor()

# Insert 100 users
for _ in range(100):
    cur.execute("""
        INSERT INTO users (name, email, created_at, last_login)
        VALUES (%s, %s, NOW() - INTERVAL '%s days', NOW())
    """, (fake.name(), fake.email(), random.randint(1, 365)))

# Insert 50 products
categories = ['Electronics', 'Books', 'Clothing', 'Home']
for _ in range(50):
    cur.execute("""
        INSERT INTO products (name, price, category)
        VALUES (%s, %s, %s)
    """, (fake.word(), round(random.uniform(10, 500), 2), random.choice(categories)))

conn.commit()
print("‚úÖ ƒê√£ t·∫°o d·ªØ li·ªáu m·∫´u!")
```

### ‚úÖ Script m√¥ ph·ªèng mua h√†ng li√™n t·ª•c (simulate_orders.py)

```python
# simulate_orders.py
import time
import random
import psycopg2

conn = psycopg2.connect(dbname="ecommerce", user="postgres", password="postgres", host="localhost")
cur = conn.cursor()

while True:
    user_id = random.randint(1, 100)
    product_id = random.randint(1, 50)
    quantity = random.randint(1, 5)
    price = round(random.uniform(10, 500), 2)

    # Insert order
    cur.execute("""
        INSERT INTO orders (user_id, total_amount, status, created_at)
        VALUES (%s, %s, 'completed', NOW())
        RETURNING order_id
    """, (user_id, quantity * price))
    order_id = cur.fetchone()[0]

    # Insert order item
    cur.execute("""
        INSERT INTO order_items (order_id, product_id, quantity, price_per_unit)
        VALUES (%s, %s, %s, %s)
    """, (order_id, product_id, quantity, price))

    conn.commit()
    print(f"üõí ƒê√£ t·∫°o ƒë∆°n h√†ng #{order_id} cho user {user_id}")
    time.sleep(3)  # 1 ƒë∆°n m·ªói 3 gi√¢y
```

> üí° Ch·∫°y script n√†y ƒë·ªÉ d·ªØ li·ªáu ‚Äúch·∫£y‚Äù li√™n t·ª•c ‚Üí Debezium b·∫Øt event ‚Üí Kafka ‚Üí Flink x·ª≠ l√Ω real-time.

---

## üîÑ PHASE 2: CDC + STREAMING PIPELINE (Tu·∫ßn 2)

### ‚úÖ C·∫•u h√¨nh Debezium (qua REST API)

```bash
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d '{
    "name": "ecommerce-connector",
    "config": {
      "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
      "database.hostname": "postgres",
      "database.port": "5432",
      "database.user": "postgres",
      "database.password": "postgres",
      "database.dbname": "ecommerce",
      "database.server.name": "ecommerce",
      "table.include.list": "public.users,public.products,public.orders,public.order_items",
      "plugin.name": "pgoutput"
    }
  }'
```

‚Üí Ki·ªÉm tra Kafka topics: `ecommerce.public.orders`, `ecommerce.public.order_items`

### ‚úÖ Flink Streaming Job (realtime_user_behavior.py)

```python
# realtime_user_behavior.py
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment

env = StreamExecutionEnvironment.get_execution_environment()
t_env = StreamTableEnvironment.create(env)

t_env.execute_sql("""
    CREATE TABLE order_events (
        order_id BIGINT,
        user_id BIGINT,
        total_amount DECIMAL(10,2),
        created_at TIMESTAMP(3),
        proc_time AS PROCTIME()
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'ecommerce.public.orders',
        'properties.bootstrap.servers' = 'localhost:9092',
        'format' = 'json',
        'scan.startup.mode' = 'earliest-offset'
    )
""")

# T√≠nh t·ªïng doanh thu theo ph√∫t
t_env.execute_sql("""
    CREATE TABLE user_behavior_sink (
        window_start TIMESTAMP(3),
        total_revenue DECIMAL(10,2)
    ) WITH (
        'connector' = 'print'
    )
""")

t_env.execute_sql("""
    INSERT INTO user_behavior_sink
    SELECT
        TUMBLE_START(proc_time, INTERVAL '1' MINUTE) as window_start,
        SUM(total_amount) as total_revenue
    FROM order_events
    GROUP BY TUMBLE(proc_time, INTERVAL '1' MINUTE)
""")
```

‚Üí Output ra console ho·∫∑c ghi v√†o Kafka topic m·ªõi: `realtime_revenue`

---

## üèûÔ∏è PHASE 3: DATA LAKEHOUSE + BATCH ANALYTICS (Tu·∫ßn 3)

### ‚úÖ Spark Job: Ghi d·ªØ li·ªáu t·ª´ Kafka ‚Üí Delta Lake (MinIO)

```python
# spark_to_deltalake.py
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("KafkaToDeltaLake") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minio") \
    .config("spark.hadoop.fs.s3a.secret.key", "minio123") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .getOrCreate()

df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "ecommerce.public.orders") \
    .load()

df_parsed = df.selectExpr("CAST(value AS STRING) as json") \
    .selectExpr("from_json(json, 'order_id LONG, user_id LONG, total_amount DOUBLE, created_at TIMESTAMP') as data") \
    .select("data.*")

df_parsed.writeStream \
    .format("delta") \
    .option("checkpointLocation", "s3a://lakehouse/checkpoints/orders") \
    .start("s3a://lakehouse/raw/orders")
```

### ‚úÖ Trino: C·∫•u h√¨nh catalog ƒë·ªÉ query Delta Lake

File: `trino/catalog/delta.properties`

```properties
connector.name=delta-lake
hive.metastore.uri=thrift://localhost:9083
delta.enable-non-concurrent-writes=true
```

‚Üí Query: `SELECT * FROM delta."s3a://lakehouse/raw/orders" LIMIT 10;`

---

## üßÆ PHASE 4: FEATURE ENGINEERING + FEATURE STORE (Tu·∫ßn 4)

### ‚úÖ Spark Feature Jobs (feature_engineering.py)

```python
# feature_engineering.py
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("FeatureEngineering").getOrCreate()

# 1. User-Item Interaction
df_interactions = spark.sql("""
    SELECT user_id, product_id, SUM(quantity) as interaction_score
    FROM delta.`s3a://lakehouse/raw/order_items` oi
    JOIN delta.`s3a://lakehouse/raw/orders` o ON oi.order_id = o.order_id
    GROUP BY user_id, product_id
""")

# 2. Product Popularity
df_popularity = spark.sql("""
    SELECT product_id, COUNT(*) as purchase_count, AVG(price_per_unit) as avg_price
    FROM delta.`s3a://lakehouse/raw/order_items`
    GROUP BY product_id
""")

# 3. User RFM
df_rfm = spark.sql("""
    SELECT user_id,
           DATEDIFF(CURRENT_DATE, MAX(created_at)) as recency,
           COUNT(*) as frequency,
           SUM(total_amount) as monetary
    FROM delta.`s3a://lakehouse/raw/orders`
    GROUP BY user_id
""")

# L∆∞u v√†o Delta Lake
df_interactions.write.format("delta").mode("overwrite").save("s3a://lakehouse/features/user_item_interactions")
df_popularity.write.format("delta").mode("overwrite").save("s3a://lakehouse/features/product_popularity")
df_rfm.write.format("delta").mode("overwrite").save("s3a://lakehouse/features/user_rfm")
```

### ‚úÖ Airflow DAG: T·ª± ƒë·ªông c·∫≠p nh·∫≠t Feature Store

```python
# dags/update_features.py
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

with DAG('update_feature_store', start_date=datetime(2025, 1, 1), schedule_interval='@daily') as dag:

    run_spark_features = BashOperator(
        task_id='run_feature_engineering',
        bash_command='spark-submit /opt/airflow/dags/feature_engineering.py'
    )

    load_to_redis = BashOperator(
        task_id='load_features_to_redis',
        bash_command='python /opt/airflow/dags/load_to_redis.py'
    )

    run_spark_features >> load_to_redis
```

### ‚úÖ Script ƒë·∫©y feature v√†o Redis (load_to_redis.py)

```python
# load_to_redis.py
import redis
import json
from pyspark.sql import SparkSession

r = redis.Redis(host='redis', port=6379, db=0)

spark = SparkSession.builder.appName("LoadToRedis").getOrCreate()

df = spark.read.format("delta").load("s3a://lakehouse/features/user_item_interactions")

for row in df.collect():
    key = f"user:{row['user_id']}:interactions"
    value = json.dumps({"product_id": row['product_id'], "score": row['interaction_score']})
    r.hset(key, row['product_id'], value)

print("‚úÖ ƒê√£ c·∫≠p nh·∫≠t feature store v√†o Redis!")
```

---

## ü§ñ PHASE 5: ML RECOMMENDATION MODEL (Tu·∫ßn 5)

### ‚úÖ Train ALS Model (train_als.py)

```python
# train_als.py
from pyspark.ml.recommendation import ALS
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("TrainALS").getOrCreate()

df = spark.read.format("delta").load("s3a://lakehouse/features/user_item_interactions")

als = ALS(
    maxIter=10,
    regParam=0.01,
    userCol="user_id",
    itemCol="product_id",
    ratingCol="interaction_score",
    coldStartStrategy="drop"
)

model = als.fit(df)
model.write().overwrite().save("s3a://lakehouse/models/als_recommender")

print("‚úÖ ƒê√£ l∆∞u m√¥ h√¨nh ALS v√†o Delta Lake!")
```

### ‚úÖ Serve Recommendation (serve_recommendation.py)

```python
# serve_recommendation.py
import redis
from pyspark.ml.recommendation import ALSModel
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("ServeRecommendation").getOrCreate()
model = ALSModel.load("s3a://lakehouse/models/als_recommender")

r = redis.Redis(host='redis', port=6379, db=0)

def recommend_for_user(user_id, top_k=5):
    user_df = spark.createDataFrame([(user_id,)], ["user_id"])
    recommendations = model.recommendForUserSubset(user_df, top_k)
    rec_list = recommendations.collect()[0]["recommendations"]
    return [r["product_id"] for r in rec_list]

# V√≠ d·ª•: G·ª£i √Ω cho user 123
print(recommend_for_user(123))
```

---

## üìà PHASE 6: DASHBOARD & MONITORING (Tu·∫ßn 6)

### ‚úÖ D√πng Trino + Metabase (ho·∫∑c Superset)

- Trino query d·ªØ li·ªáu t·ª´ Delta Lake ‚Üí Metabase visualize
- Dashboard g·ªìm:
  - Doanh thu theo th·ªùi gian
  - Top s·∫£n ph·∫©m b√°n ch·∫°y
  - Ph√¢n b·ªë kh√°ch h√†ng theo RFM
  - S·ªë l∆∞·ª£ng ƒë·ªÅ xu·∫•t ƒë∆∞·ª£c sinh ra

### ‚úÖ Log & Alert (Airflow)

- G·ª≠i Slack/Email n·∫øu job fail
- Ghi log v√†o file ho·∫∑c Elasticsearch (optional)

---

## üìÅ PHASE 7: DOCUMENT & PRESENT (Tu·∫ßn 7)

### ‚úÖ GitHub Repo Structure

```
/customer-insight-platform/
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ generate_data.py
‚îÇ   ‚îî‚îÄ‚îÄ simulate_orders.py
‚îú‚îÄ‚îÄ streaming/
‚îÇ   ‚îî‚îÄ‚îÄ realtime_user_behavior.py
‚îú‚îÄ‚îÄ batch/
‚îÇ   ‚îî‚îÄ‚îÄ feature_engineering.py
‚îú‚îÄ‚îÄ ml/
‚îÇ   ‚îú‚îÄ‚îÄ train_als.py
‚îÇ   ‚îî‚îÄ‚îÄ serve_recommendation.py
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îú‚îÄ‚îÄ update_features.py
‚îÇ   ‚îî‚îÄ‚îÄ load_to_redis.py
‚îî‚îÄ‚îÄ docs/
    ‚îî‚îÄ‚îÄ architecture.png
```

### ‚úÖ README.md ph·∫£i c√≥:

- M√¥ t·∫£ d·ª± √°n
- C√°ch ch·∫°y (docker-compose up, python scripts, airflow)
- Screenshot dashboard
- Ki·∫øn tr√∫c h·ªá th·ªëng (v·∫Ω b·∫±ng draw.io ho·∫∑c Excalidraw)

---

## üéì KHI PH·ªéNG V·∫§N ‚Äî B·∫†N N√ìI G√å?

> ‚ÄúT√¥i ƒë√£ t·ª± x√¢y d·ª±ng m·ªôt h·ªá th·ªëng ph√¢n t√≠ch h√†nh vi kh√°ch h√†ng v√† g·ª£i √Ω s·∫£n ph·∫©m end-to-end, bao g·ªìm:  
> - Thu th·∫≠p d·ªØ li·ªáu real-time t·ª´ PostgreSQL qua Debezium + Kafka  
> - X·ª≠ l√Ω streaming b·∫±ng Flink  
> - L∆∞u tr·ªØ v√†o Delta Lake tr√™n MinIO  
> - T·ª± ƒë·ªông h√≥a b·∫±ng Airflow  
> - X√¢y feature store v·ªõi Redis + Delta Lake  
> - Train m√¥ h√¨nh ALS recommendation b·∫±ng Spark MLlib  
> - Serve ƒë·ªÅ xu·∫•t real-time  
> T·∫•t c·∫£ ch·∫°y local v·ªõi Docker ‚Äî kh√¥ng c·∫ßn cloud.‚Äù

---

## ‚úÖ BONUS: N√ÇNG C·∫§P SAU N√ÄY (N·∫øu c√≥ th·ªùi gian)

- Th√™m Kafka Streams thay Flink
- D√πng Feast l√†m Feature Store chuy√™n nghi·ªáp
- Deploy model v·ªõi FastAPI + Docker
- Th√™m Grafana ƒë·ªÉ monitor pipeline
- D√πng dbt ƒë·ªÉ transform data

---

## üöÄ B·∫†N MU·ªêN M√åNH G·ª¨I G√å TI·∫æP THEO?

üëâ M√¨nh c√≥ th·ªÉ g·ª≠i b·∫°n ngay:

- File `docker-compose.yml` ho√†n ch·ªânh
- Script `generate_data.py`, `simulate_orders.py`
- Code Flink + Spark + Airflow m·∫´u
- File c·∫•u h√¨nh Trino + MinIO
- README.md m·∫´u chuy√™n nghi·ªáp

Ch·ªâ c·∫ßn n√≥i: **‚ÄúG·ª≠i code m·∫´u‚Äù** ‚Äî m√¨nh s·∫Ω zip v√† g·ª≠i t·ª´ng ph·∫ßn theo th·ª© t·ª± b·∫°n c·∫ßn!

---

B·∫°n ƒë√£ s·∫µn s√†ng ƒë·ªÉ b·∫Øt ƒë·∫ßu ch∆∞a? M√¨nh s·∫Ω ƒë·ªìng h√†nh c√πng b·∫°n t·ª´ng b∆∞·ªõc üòä