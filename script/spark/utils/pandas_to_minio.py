import os
import pandas as pd
import psycopg2
from minio import Minio
from io import BytesIO
import shutil
from pathlib import Path
from deltalake.writer import write_deltalake
from dotenv import load_dotenv
load_dotenv()

# Config
MINIO_ENDPOINT = f"localhost:{os.getenv('MINIO_PORT')}"
MINIO_ACCESS_KEY = os.getenv("MINIO_ACCESS_KEY")
MINIO_SECRET_KEY = os.getenv("MINIO_SECRET_KEY")
LOCAL_DELTA_PATH = "./script/spark/delta_data"  # Local Delta Lake path
BUCKET_NAME = "e-commerece"
FOLDER_NAME = "data-postgres"

DB_CONFIG = {
    'host': 'localhost',
    'port': os.getenv('POSTGRES_PORT'),
    'database': os.getenv('POSTGRES_DB'),
    'user': os.getenv('POSTGRES_USER'),
    'password': os.getenv('POSTGRES_PASSWORD')
}

TABLES = ["orders", "order_items", "payments", "customers", "products"]

def create_minio_client():
    return Minio(
        endpoint=MINIO_ENDPOINT,
        access_key=MINIO_ACCESS_KEY,
        secret_key=MINIO_SECRET_KEY,
        secure=False
    )

def create_bucket_if_not_exists(client, bucket_name):
    if not client.bucket_exists(bucket_name):
        client.make_bucket(bucket_name)
        print(f"‚úÖ Created bucket: {bucket_name}")
    else:
        print(f"üì¶ Bucket {bucket_name} already exists")

def clean_dataframe_for_delta(df, table_name):
    """Clean DataFrame ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi Delta Lake"""
    print(f"üßπ Cleaning DataFrame for table {table_name}")
    
    df_cleaned = df.copy()
    
    # X·ª≠ l√Ω c√°c columns to√†n NULL
    for col in df_cleaned.columns:
        if df_cleaned[col].isnull().all():
            print(f"‚ö†Ô∏è Column '{col}' is all NULL - converting to string type")
            df_cleaned[col] = df_cleaned[col].astype('object').fillna('')
        elif df_cleaned[col].dtype == 'object':
            # ƒê·∫£m b·∫£o object columns kh√¥ng c√≥ mixed types
            df_cleaned[col] = df_cleaned[col].astype(str).replace('nan', '')
    
    # Convert problematic data types
    for col in df_cleaned.columns:
        dtype = df_cleaned[col].dtype
        
        # Handle datetime columns
        if 'datetime' in str(dtype):
            df_cleaned[col] = pd.to_datetime(df_cleaned[col], errors='coerce')
        
        # Handle mixed int/float with nulls
        elif dtype == 'object' and col not in ['created_at', 'updated_at']:
            # Try to convert to numeric if possible
            try:
                numeric_series = pd.to_numeric(df_cleaned[col], errors='coerce')
                if not numeric_series.isnull().all():
                    df_cleaned[col] = numeric_series
            except:
                pass
    
    print(f"‚úÖ Cleaned DataFrame shape: {df_cleaned.shape}")
    return df_cleaned

def upload_delta_table_to_minio(minio_client, local_path, bucket_name, table_name):
    """Upload to√†n b·ªô Delta Lake table (bao g·ªìm _delta_log) l√™n MinIO"""
    local_path = Path(local_path)
    
    if not local_path.exists():
        print(f"‚ùå Local path {local_path} kh√¥ng t·ªìn t·∫°i")
        return False
        
    # Upload t·∫•t c·∫£ files trong Delta table
    for file_path in local_path.rglob('*'):
        if file_path.is_file():
            # T·∫°o relative path cho MinIO object
            relative_path = file_path.relative_to(local_path)
            object_name = f"{FOLDER_NAME}/{table_name}/{relative_path}"
            
            try:
                minio_client.fput_object(
                    bucket_name=bucket_name,
                    object_name=object_name,
                    file_path=str(file_path)
                )
                print(f"üì§ Uploaded: {object_name}")
            except Exception as e:
                print(f"‚ùå Failed to upload {file_path}: {e}")
                return False
                
    return True

def pandas_to_minio():
    # T·∫°o local delta directory
    os.makedirs(LOCAL_DELTA_PATH, exist_ok=True)
    
    # K·∫øt n·ªëi MinIO
    minio_client = create_minio_client()
    create_bucket_if_not_exists(minio_client, BUCKET_NAME)
    
    # K·∫øt n·ªëi PostgreSQL
    conn = psycopg2.connect(**DB_CONFIG)
    
    for table in TABLES:
        print(f"üîÑ Processing table: {table}")
        
        try:
            # ƒê·ªçc data b·∫±ng pandas
            df = pd.read_sql(f"SELECT * FROM {table}", conn)
            
            if len(df) == 0:
                print(f"‚ö†Ô∏è Table {table} is empty, skipping...")
                continue
            
            # Clean DataFrame cho Delta Lake
            df_cleaned = clean_dataframe_for_delta(df, table)
            
            # T·∫°o Delta Lake table local
            table_path = os.path.join(LOCAL_DELTA_PATH, table)
            print(f"üíæ Writing Delta Lake table to {table_path}")
            
            # X√≥a table c≈© n·∫øu c√≥
            if os.path.exists(table_path):
                shutil.rmtree(table_path)
            
            # Vi·∫øt Delta Lake table v·ªõi cleaned data
            write_deltalake(
                table_or_uri=table_path,
                data=df_cleaned,
                mode="overwrite",
                overwrite_schema=True
            )
            
            # Upload to√†n b·ªô Delta table l√™n MinIO
            success = upload_delta_table_to_minio(
                minio_client=minio_client,
                local_path=table_path,
                bucket_name=BUCKET_NAME,
                table_name=table
            )
            
            if success:
                print(f"‚úÖ Successfully uploaded {table} Delta table to MinIO")
            else:
                print(f"‚ùå Failed to upload {table}")
                
        except Exception as e:
            print(f"‚ùå Error processing table {table}: {str(e)}")
            continue
    
    conn.close()
    print(f"üéâ All tables processed! Local Delta files: {LOCAL_DELTA_PATH}")

if __name__ == "__main__":
    pandas_to_minio()